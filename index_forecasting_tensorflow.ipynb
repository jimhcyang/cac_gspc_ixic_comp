{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1ed09ab",
      "metadata": {},
      "source": [
        "# SP500 / NASDAQ Rolling Forecast Notebook\n",
        "\n",
        "Paired notebook design for side-by-side TensorFlow vs PyTorch comparison.\n",
        "\n",
        "Key rules applied:\n",
        "- Plus1 rolling notation (`N - W + 1`).\n",
        "- Refit per window.\n",
        "- CSV-first logging.\n",
        "- Tuning selection by **MAPE** (R2 is secondary diagnostics)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427de0ae",
      "metadata": {},
      "source": [
        "## Window Notation (Plus1)\n",
        "\n",
        "For each `W`:\n",
        "- `TEST_LEN = max(1, round(W * test_ratio))`\n",
        "- `win   = df[start : start+W]`\n",
        "- `train = win[:W-TEST_LEN]`\n",
        "- `test  = win[W-TEST_LEN:]`\n",
        "\n",
        "Loop:\n",
        "- `for start in range(0, N - W + 1, STEP)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e6ad50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed:\n",
        "# %pip install yfinance pandas numpy scikit-learn matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112e11b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "FRAMEWORK_NAME = 'tensorflow'\n",
        "\n",
        "# ----------------------------\n",
        "# Dates and data scope\n",
        "# ----------------------------\n",
        "START_DATE = '2024-01-01'\n",
        "END_DATE = '2025-12-31'\n",
        "TUNING_END_DATE = '2024-04-30'\n",
        "BUFFER_MONTHS = 6  # extra history for stable technical indicators\n",
        "\n",
        "# ----------------------------\n",
        "# Rolling settings\n",
        "# ----------------------------\n",
        "WINDOW_SIZES = [10, 20]\n",
        "TEST_RATIO_BY_WINDOW = {10: 0.10, 20: 0.05}\n",
        "STEP = 1\n",
        "\n",
        "# ----------------------------\n",
        "# Smoke test controls\n",
        "# ----------------------------\n",
        "SMOKE_TEST = True\n",
        "SMOKE_MAX_CONFIGS = 1\n",
        "SMOKE_MAX_WINDOWS = 2\n",
        "SMOKE_EPOCH = 2\n",
        "\n",
        "MAX_CONFIGS_PER_MODEL = None  # None means full grid\n",
        "\n",
        "# ----------------------------\n",
        "# Logging\n",
        "# ----------------------------\n",
        "LOG_BASE = Path('dl_logs_index') / FRAMEWORK_NAME\n",
        "LOG_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_CACHE_DIR = Path('data_cache') / 'index_ohlcv'\n",
        "RAW_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET_COL = 'y'\n",
        "DATASETS = ['SP500', 'NASDAQ']\n",
        "\n",
        "print('Framework:', FRAMEWORK_NAME)\n",
        "print('Log base: ', LOG_BASE.resolve())\n",
        "print('Smoke test enabled:', SMOKE_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b0ef64",
      "metadata": {},
      "outputs": [],
      "source": [
        "INDEX_META = {\n",
        "    'SP500': {'symbol': '^GSPC', 'prefix': 'sp500', 'alias': 'SP500'},\n",
        "    'NASDAQ': {'symbol': '^IXIC', 'prefix': 'nasdaq', 'alias': 'NASDAQ'},\n",
        "}\n",
        "\n",
        "\n",
        "def _cache_path(alias: str, start: str, end: str) -> Path:\n",
        "    safe = str(alias).strip().upper()\n",
        "    return RAW_CACHE_DIR / f'{safe}_{start}_{end}.csv'\n",
        "\n",
        "\n",
        "def _is_valid_ohlcv_df(df: pd.DataFrame) -> bool:\n",
        "    if df is None or df.empty:\n",
        "        return False\n",
        "    need = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    if any(c not in df.columns for c in need):\n",
        "        return False\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        return False\n",
        "    if df.index.isna().all():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def _read_cache(cache_path: Path) -> pd.DataFrame | None:\n",
        "    try:\n",
        "        df = pd.read_csv(cache_path, index_col=0, parse_dates=True)\n",
        "        df.index.name = None\n",
        "        if _is_valid_ohlcv_df(df):\n",
        "            return df\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def _download_ohlcv(symbol: str, start: str, end: str, alias: str) -> pd.DataFrame:\n",
        "    cache_path = _cache_path(alias, start, end)\n",
        "\n",
        "    if cache_path.exists():\n",
        "        cached = _read_cache(cache_path)\n",
        "        if cached is not None:\n",
        "            print(f'[CACHE] load {alias} <- {cache_path}')\n",
        "            return cached[['Open', 'High', 'Low', 'Close', 'Volume']].dropna().sort_index()\n",
        "\n",
        "        print(f'[CACHE] invalid/empty cache for {alias}; redownloading and replacing {cache_path}')\n",
        "        try:\n",
        "            cache_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f'[CACHE] miss {alias}; downloading...')\n",
        "    df = yf.download(symbol, start=start, end=end, progress=False, auto_adjust=False)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [c[0] for c in df.columns]\n",
        "\n",
        "    if not _is_valid_ohlcv_df(df):\n",
        "        raise ValueError(f'Downloaded data for {symbol} is empty/invalid for {start}..{end}')\n",
        "\n",
        "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna().sort_index()\n",
        "    if df.empty:\n",
        "        raise ValueError(f'Downloaded data for {symbol} became empty after cleanup ({start}..{end})')\n",
        "\n",
        "    df.to_csv(cache_path)\n",
        "    print(f'[CACHE] save {alias} -> {cache_path}')\n",
        "    return df\n",
        "\n",
        "\n",
        "def _rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
        "    d = close.diff()\n",
        "    up = d.clip(lower=0)\n",
        "    dn = -d.clip(upper=0)\n",
        "    ma_up = up.ewm(alpha=1 / period, adjust=False).mean()\n",
        "    ma_dn = dn.ewm(alpha=1 / period, adjust=False).mean()\n",
        "    rs = ma_up / ma_dn.replace(0, np.nan)\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "\n",
        "def build_single_index_df(index_key: str) -> pd.DataFrame:\n",
        "    meta = INDEX_META[index_key]\n",
        "    symbol = meta['symbol']\n",
        "    prefix = meta['prefix']\n",
        "    alias = meta['alias']\n",
        "\n",
        "    # buffer window for technical computation\n",
        "    buffered_start = (pd.Timestamp(START_DATE) - pd.DateOffset(months=BUFFER_MONTHS)).strftime('%Y-%m-%d')\n",
        "    raw = _download_ohlcv(symbol, buffered_start, END_DATE, alias)\n",
        "\n",
        "    df = pd.DataFrame(index=raw.index)\n",
        "    close = raw['Close']\n",
        "\n",
        "    # Five technical indicators based solely on this index\n",
        "    df[f'{prefix}_rsi14'] = _rsi(close, 14)\n",
        "\n",
        "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
        "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
        "    macd = ema12 - ema26\n",
        "    sig = macd.ewm(span=9, adjust=False).mean()\n",
        "    df[f'{prefix}_macd_hist'] = macd - sig\n",
        "\n",
        "    ma20 = close.rolling(20).mean()\n",
        "    sd20 = close.rolling(20).std()\n",
        "    df[f'{prefix}_bb_z20'] = (close - ma20) / sd20.replace(0, np.nan)\n",
        "\n",
        "    df[f'{prefix}_roc10'] = close.pct_change(10)\n",
        "    df[f'{prefix}_vol20'] = close.pct_change().rolling(20).std()\n",
        "\n",
        "    # target: next-day same-index close\n",
        "    df[TARGET_COL] = close.shift(-1)\n",
        "\n",
        "    # trim to pipeline horizon after techs are computed\n",
        "    df = df.loc[START_DATE:END_DATE].dropna().copy()\n",
        "    return df\n",
        "\n",
        "\n",
        "def feature_cols_for(index_key: str) -> List[str]:\n",
        "    p = INDEX_META[index_key]['prefix']\n",
        "    return [f'{p}_rsi14', f'{p}_macd_hist', f'{p}_bb_z20', f'{p}_roc10', f'{p}_vol20']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9103bd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def iter_windows_plus1(df: pd.DataFrame, window_size: int, test_ratio: float, step: int = 1):\n",
        "    test_len = max(1, int(round(window_size * test_ratio)))\n",
        "    n = len(df)\n",
        "    for start in range(0, n - window_size + 1, step):\n",
        "        win = df.iloc[start:start + window_size]\n",
        "        yield {\n",
        "            'start': start,\n",
        "            'end_exclusive': start + window_size,\n",
        "            'test_len': test_len,\n",
        "            'train_df': win.iloc[:window_size - test_len],\n",
        "            'test_df': win.iloc[window_size - test_len:],\n",
        "        }\n",
        "\n",
        "\n",
        "def scale_xy_window(train_df: pd.DataFrame, test_df: pd.DataFrame, feature_cols: List[str], target_col: str):\n",
        "    X_tr = train_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    X_te = test_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_tr = train_df[target_col].to_numpy(dtype=np.float32)\n",
        "    y_te = test_df[target_col].to_numpy(dtype=np.float32)\n",
        "\n",
        "    sx = StandardScaler().fit(X_tr)\n",
        "    X_tr_s = sx.transform(X_tr).astype(np.float32)\n",
        "    X_te_s = sx.transform(X_te).astype(np.float32)\n",
        "\n",
        "    sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "    y_tr_s = sy.transform(y_tr.reshape(-1, 1)).ravel().astype(np.float32)\n",
        "    y_te_s = sy.transform(y_te.reshape(-1, 1)).ravel().astype(np.float32)\n",
        "\n",
        "    return X_tr_s, X_te_s, y_tr_s, y_te_s, sy\n",
        "\n",
        "\n",
        "def parse_list_like(x: Any) -> List[float]:\n",
        "    if isinstance(x, list):\n",
        "        return [float(v) for v in x]\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return []\n",
        "    s = str(x)\n",
        "    for fn in (ast.literal_eval, json.loads):\n",
        "        try:\n",
        "            v = fn(s)\n",
        "            if isinstance(v, list):\n",
        "                return [float(z) for z in v]\n",
        "            return [float(v)]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "\n",
        "def mape_r2(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "    yt = y_true[mask]\n",
        "    yp = y_pred[mask]\n",
        "    if len(yt) == 0:\n",
        "        return {'MAPE': np.nan, 'R2': np.nan, 'n_points': 0}\n",
        "    mape = float(mean_absolute_percentage_error(yt, yp) * 100.0)\n",
        "    r2 = float(r2_score(yt, yp)) if len(yt) > 1 else np.nan\n",
        "    return {'MAPE': mape, 'R2': r2, 'n_points': int(len(yt))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6441d8e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Requested grid\n",
        "BASE_GRID = {\n",
        "    'hidden_dim': [32, 64],\n",
        "    'num_layers': [2, 3],\n",
        "    'dropout': [0.0, 0.2],\n",
        "    'learning_rate': [1e-3, 1e-4],\n",
        "    'batch_size': [32, 64],\n",
        "    'epoch': [32, 64],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'tcn_dilation_rate': [2, 4],\n",
        "    'transformer_attention heads': [2, 4],\n",
        "}\n",
        "\n",
        "\n",
        "def expand_grid(d: Dict[str, List[Any]]) -> List[Dict[str, Any]]:\n",
        "    keys = list(d.keys())\n",
        "    vals = [d[k] for k in keys]\n",
        "    out: List[Dict[str, Any]] = []\n",
        "\n",
        "    def rec(i: int, cur: Dict[str, Any]):\n",
        "        if i == len(keys):\n",
        "            out.append(dict(cur))\n",
        "            return\n",
        "        k = keys[i]\n",
        "        for v in vals[i]:\n",
        "            cur[k] = v\n",
        "            rec(i + 1, cur)\n",
        "\n",
        "    rec(0, {})\n",
        "    return out\n",
        "\n",
        "\n",
        "CORE_KEYS = ['hidden_dim', 'num_layers', 'dropout', 'learning_rate', 'batch_size', 'epoch', 'activation']\n",
        "TCN_KEYS = CORE_KEYS + ['tcn_dilation_rate']\n",
        "TRANS_KEYS = CORE_KEYS + ['transformer_attention heads']\n",
        "\n",
        "MLP_GRID = expand_grid({k: BASE_GRID[k] for k in CORE_KEYS})\n",
        "RNN_GRID = expand_grid({k: BASE_GRID[k] for k in CORE_KEYS})\n",
        "LSTM_GRID = expand_grid({k: BASE_GRID[k] for k in CORE_KEYS})\n",
        "GRU_GRID = expand_grid({k: BASE_GRID[k] for k in CORE_KEYS})\n",
        "TCN_GRID = expand_grid({k: BASE_GRID[k] for k in TCN_KEYS})\n",
        "TRANSFORMER_GRID = expand_grid({k: BASE_GRID[k] for k in TRANS_KEYS})\n",
        "\n",
        "print('Grid sizes -> MLP/RNN/LSTM/GRU:', len(MLP_GRID), 'TCN:', len(TCN_GRID), 'Transformer:', len(TRANSFORMER_GRID))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68eda83f",
      "metadata": {},
      "source": [
        "## Framework-Specific Cell (Different Between Notebooks)\n",
        "\n",
        "The next code cell is the only implementation-level difference (training backend)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6838bb47",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Device priority: CUDA -> MPS (Metal) -> CPU\n",
        "_tf_gpus = tf.config.list_physical_devices('GPU')\n",
        "if _tf_gpus:\n",
        "    TF_DEVICE = '/GPU:0'\n",
        "    TF_DEVICE_NAME = _tf_gpus[0].name\n",
        "else:\n",
        "    TF_DEVICE = '/CPU:0'\n",
        "    TF_DEVICE_NAME = 'CPU'\n",
        "print(f'[TF] Device selected: {TF_DEVICE} ({TF_DEVICE_NAME})')\n",
        "\n",
        "\n",
        "def build_tf_model(model_name: str, n_features: int, hp: Dict[str, Any]):\n",
        "    h = int(hp['hidden_dim'])\n",
        "    n = int(hp['num_layers'])\n",
        "    d = float(hp['dropout'])\n",
        "    act = hp['activation']\n",
        "    tcn_d = int(hp.get('tcn_dilation_rate', 2))\n",
        "    n_heads = int(hp.get('transformer_attention heads', 2))\n",
        "\n",
        "    x_in = keras.Input(shape=(n_features,))\n",
        "\n",
        "    if model_name == 'MLP':\n",
        "        x = x_in\n",
        "        for _ in range(n):\n",
        "            x = layers.Dense(h, activation=act)(x)\n",
        "            x = layers.Dropout(d)(x)\n",
        "\n",
        "    elif model_name in {'RNN', 'LSTM', 'GRU'}:\n",
        "        x = layers.Reshape((1, n_features))(x_in)\n",
        "        for i in range(n):\n",
        "            rs = i < (n - 1)\n",
        "            if model_name == 'RNN':\n",
        "                x = layers.SimpleRNN(h, activation=act, return_sequences=rs)(x)\n",
        "            elif model_name == 'LSTM':\n",
        "                x = layers.LSTM(h, activation=act, return_sequences=rs)(x)\n",
        "            else:\n",
        "                x = layers.GRU(h, activation=act, return_sequences=rs)(x)\n",
        "            x = layers.Dropout(d)(x)\n",
        "\n",
        "    elif model_name == 'TCN':\n",
        "        x = layers.Reshape((1, n_features))(x_in)\n",
        "        for _ in range(n):\n",
        "            x = layers.Conv1D(h, kernel_size=2, padding='causal', dilation_rate=tcn_d, activation=act)(x)\n",
        "            x = layers.Dropout(d)(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    elif model_name == 'Transformer':\n",
        "        x = layers.Reshape((1, n_features))(x_in)\n",
        "        d_model = max(h, n_heads)\n",
        "        x = layers.Dense(d_model)(x)\n",
        "        for _ in range(n):\n",
        "            a = layers.MultiHeadAttention(num_heads=n_heads, key_dim=max(1, d_model // n_heads), dropout=d)(x, x)\n",
        "            x = layers.LayerNormalization()(x + a)\n",
        "            ff = layers.Dense(h, activation=act)(x)\n",
        "            ff = layers.Dropout(d)(ff)\n",
        "            ff = layers.Dense(d_model)(ff)\n",
        "            x = layers.LayerNormalization()(x + ff)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(model_name)\n",
        "\n",
        "    y = layers.Dense(1)(x)\n",
        "    m = keras.Model(x_in, y)\n",
        "    m.compile(optimizer=keras.optimizers.Adam(learning_rate=float(hp['learning_rate'])), loss='mse')\n",
        "    return m\n",
        "\n",
        "\n",
        "def fit_predict_window_tf(model_name: str, hp: Dict[str, Any], X_tr_s, y_tr_s, X_te_s):\n",
        "    hp_run = dict(hp)\n",
        "    if SMOKE_TEST:\n",
        "        hp_run['epoch'] = min(int(hp_run['epoch']), int(SMOKE_EPOCH))\n",
        "\n",
        "    with tf.device(TF_DEVICE):\n",
        "        m = build_tf_model(model_name, X_tr_s.shape[1], hp_run)\n",
        "        m.fit(X_tr_s, y_tr_s, epochs=int(hp_run['epoch']), batch_size=int(hp_run['batch_size']), verbose=0)\n",
        "        yp = m.predict(X_te_s, verbose=0).reshape(-1)\n",
        "    return yp\n",
        "\n",
        "print('Defined: fit_predict_window_tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7722ce2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Rolling run + logging\n",
        "# ----------------------------\n",
        "\n",
        "def _log_path(dataset_tag: str, model_name: str, w: int) -> Path:\n",
        "    d = LOG_BASE / dataset_tag / f'w{int(w)}' / model_name.upper()\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    return d / f'{model_name.upper()}_{dataset_tag}.csv'\n",
        "\n",
        "\n",
        "def run_grid_and_log(\n",
        "    *,\n",
        "    dataset_tag: str,\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    model_name: str,\n",
        "    model_grid: List[Dict[str, Any]],\n",
        "    window_size: int,\n",
        "    test_ratio: float,\n",
        "    tuning_end_date: str,\n",
        "    overwrite: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    log_path = _log_path(dataset_tag, model_name, window_size)\n",
        "\n",
        "    grid = model_grid\n",
        "    if MAX_CONFIGS_PER_MODEL is not None:\n",
        "        grid = grid[:MAX_CONFIGS_PER_MODEL]\n",
        "    if SMOKE_TEST:\n",
        "        grid = grid[:SMOKE_MAX_CONFIGS]\n",
        "\n",
        "    if overwrite and log_path.exists():\n",
        "        log_path.unlink()\n",
        "\n",
        "    rows = []\n",
        "    tune_cut = pd.Timestamp(tuning_end_date)\n",
        "\n",
        "    for cfg_idx, hp in enumerate(tqdm(grid, desc=f'{dataset_tag}-{model_name}-w{window_size}', unit='cfg')):\n",
        "        w_iter = iter_windows_plus1(df, window_size, test_ratio, STEP)\n",
        "        for win_id, win in enumerate(w_iter, start=1):\n",
        "            if SMOKE_TEST and win_id > SMOKE_MAX_WINDOWS:\n",
        "                break\n",
        "\n",
        "            tr_df = win['train_df']\n",
        "            te_df = win['test_df']\n",
        "            if tr_df.empty or te_df.empty:\n",
        "                continue\n",
        "\n",
        "            X_tr_s, X_te_s, y_tr_s, y_te_s, sy = scale_xy_window(tr_df, te_df, feature_cols, TARGET_COL)\n",
        "\n",
        "            if FRAMEWORK_NAME == 'tensorflow':\n",
        "                y_pred_s = fit_predict_window_tf(model_name, hp, X_tr_s, y_tr_s, X_te_s)\n",
        "            else:\n",
        "                y_pred_s = fit_predict_window_torch(model_name, hp, X_tr_s, y_tr_s, X_te_s)\n",
        "\n",
        "            y_pred = sy.inverse_transform(np.asarray(y_pred_s).reshape(-1, 1)).ravel()\n",
        "            y_true = sy.inverse_transform(np.asarray(y_te_s).reshape(-1, 1)).ravel()\n",
        "\n",
        "            rows.append({\n",
        "                'framework': FRAMEWORK_NAME,\n",
        "                'dataset_tag': dataset_tag,\n",
        "                'model_name': model_name.upper(),\n",
        "                'config_index': int(cfg_idx),\n",
        "                'model_hyperparameters_dict': json.dumps(hp, sort_keys=True),\n",
        "                'window_id': int(win_id),\n",
        "                'window_size': int(window_size),\n",
        "                'test_ratio': float(test_ratio),\n",
        "                'test_len': int(win['test_len']),\n",
        "                'start_date': str(tr_df.index[0].date()),\n",
        "                'end_date': str(te_df.index[-1].date()),\n",
        "                'is_tuning_window': bool(pd.Timestamp(te_df.index[-1]) <= tune_cut),\n",
        "                'test_data_values_list': json.dumps([float(v) for v in y_true.tolist()]),\n",
        "                'test_data_model_predictions_list': json.dumps([float(v) for v in y_pred.tolist()]),\n",
        "            })\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "    if not out.empty:\n",
        "        if log_path.exists() and not overwrite:\n",
        "            old = pd.read_csv(log_path)\n",
        "            out = pd.concat([old, out], ignore_index=True)\n",
        "        out.to_csv(log_path, index=False)\n",
        "\n",
        "    print(f'[INFO] {dataset_tag} {model_name} w{window_size} rows={len(rows)} -> {log_path}')\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0db6792",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build datasets independently (SP500-only and NASDAQ-only)\n",
        "DATA_BY_INDEX = {k: build_single_index_df(k) for k in DATASETS}\n",
        "FEATURES_BY_INDEX = {k: feature_cols_for(k) for k in DATASETS}\n",
        "\n",
        "for k in DATASETS:\n",
        "    d = DATA_BY_INDEX[k]\n",
        "    f = FEATURES_BY_INDEX[k]\n",
        "\n",
        "    if d.empty or not isinstance(d.index, pd.DatetimeIndex):\n",
        "        print(f'{k}: rows=0, features={f}, range=<empty>')\n",
        "        continue\n",
        "\n",
        "    idx_min = d.index.min()\n",
        "    idx_max = d.index.max()\n",
        "    left = idx_min.date() if pd.notna(idx_min) else '<na>'\n",
        "    right = idx_max.date() if pd.notna(idx_max) else '<na>'\n",
        "    print(f'{k}: rows={len(d)}, features={f}, range={left} -> {right}')\n",
        "\n",
        "    for W in WINDOW_SIZES:\n",
        "        ratio = TEST_RATIO_BY_WINDOW[W]\n",
        "        test_len = max(1, int(round(W * ratio)))\n",
        "        n_windows = max(0, (len(d) - W) // STEP + 1)\n",
        "        print(f'  [PLUS1] W={W}, ratio={ratio:.4f}, TEST_LEN={test_len}, windows={n_windows}, loop=range(0, N-{W}+1, {STEP})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f239e1ee",
      "metadata": {},
      "source": [
        "## Optional Smoke Test Cell\n",
        "\n",
        "This quick check runs one model/config/window branch so you can verify end-to-end logging before full sweeps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d2022e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SMOKE_TEST:\n",
        "    _ = run_grid_and_log(\n",
        "        dataset_tag='SP500',\n",
        "        df=DATA_BY_INDEX['SP500'],\n",
        "        feature_cols=FEATURES_BY_INDEX['SP500'],\n",
        "        model_name='MLP',\n",
        "        model_grid=MLP_GRID,\n",
        "        window_size=10,\n",
        "        test_ratio=TEST_RATIO_BY_WINDOW[10],\n",
        "        tuning_end_date=TUNING_END_DATE,\n",
        "        overwrite=True,\n",
        "    )\n",
        "else:\n",
        "    print('SMOKE_TEST is False; skipping smoke run.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e284511c",
      "metadata": {},
      "source": [
        "## Model Runs (Separated)\n",
        "\n",
        "No outer `for model in MODELS` loop; each model has its own cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2f0248",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='MLP',\n",
        "            model_grid=MLP_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8c46a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RNN\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='RNN',\n",
        "            model_grid=RNN_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf98648",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='LSTM',\n",
        "            model_grid=LSTM_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e751c4ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRU\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='GRU',\n",
        "            model_grid=GRU_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5011132a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TCN\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='TCN',\n",
        "            model_grid=TCN_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5a0214",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer\n",
        "for ds in DATASETS:\n",
        "    for W in WINDOW_SIZES:\n",
        "        run_grid_and_log(\n",
        "            dataset_tag=ds,\n",
        "            df=DATA_BY_INDEX[ds],\n",
        "            feature_cols=FEATURES_BY_INDEX[ds],\n",
        "            model_name='Transformer',\n",
        "            model_grid=TRANSFORMER_GRID,\n",
        "            window_size=W,\n",
        "            test_ratio=TEST_RATIO_BY_WINDOW[W],\n",
        "            tuning_end_date=TUNING_END_DATE,\n",
        "            overwrite=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f663e86c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score logs by (dataset_tag, model, window_size, config_index)\n",
        "\n",
        "def collect_log_files(base: Path) -> List[Path]:\n",
        "    return sorted([p for p in base.glob('*/*/*/*.csv') if p.is_file()])\n",
        "\n",
        "\n",
        "def score_one_log(path: Path) -> pd.DataFrame:\n",
        "    d = pd.read_csv(path)\n",
        "    if d.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    out = []\n",
        "    gobj = d.groupby(['dataset_tag', 'model_name', 'window_size', 'config_index'], dropna=True)\n",
        "    for (dataset_tag, model_name, w, cfg_idx), g in gobj:\n",
        "        g = g.sort_values('window_id', kind='stable')\n",
        "\n",
        "        yt_tune, yp_tune = [], []\n",
        "        yt_eval, yp_eval = [], []\n",
        "        hp = g['model_hyperparameters_dict'].dropna().iloc[0] if 'model_hyperparameters_dict' in g.columns else '{}'\n",
        "\n",
        "        for _, r in g.iterrows():\n",
        "            yt = parse_list_like(r.get('test_data_values_list'))\n",
        "            yp = parse_list_like(r.get('test_data_model_predictions_list'))\n",
        "            if not yt or not yp:\n",
        "                continue\n",
        "            if bool(r.get('is_tuning_window')):\n",
        "                yt_tune.extend(yt)\n",
        "                yp_tune.extend(yp)\n",
        "            else:\n",
        "                yt_eval.extend(yt)\n",
        "                yp_eval.extend(yp)\n",
        "\n",
        "        m_tune = mape_r2(np.asarray(yt_tune, dtype=float), np.asarray(yp_tune, dtype=float))\n",
        "        m_eval = mape_r2(np.asarray(yt_eval, dtype=float), np.asarray(yp_eval, dtype=float))\n",
        "\n",
        "        out.append({\n",
        "            'framework': FRAMEWORK_NAME,\n",
        "            'dataset_tag': dataset_tag,\n",
        "            'model': str(model_name).upper(),\n",
        "            'window_size': int(w),\n",
        "            'config_index': int(cfg_idx),\n",
        "            'hyperparams_json': hp,\n",
        "            'tune_MAPE': m_tune['MAPE'],\n",
        "            'tune_R2': m_tune['R2'],\n",
        "            'tune_n': m_tune['n_points'],\n",
        "            'eval_MAPE': m_eval['MAPE'],\n",
        "            'eval_R2': m_eval['R2'],\n",
        "            'eval_n': m_eval['n_points'],\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "parts = []\n",
        "for fp in collect_log_files(LOG_BASE):\n",
        "    s = score_one_log(fp)\n",
        "    if not s.empty:\n",
        "        parts.append(s)\n",
        "\n",
        "scores_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
        "print('Score rows:', len(scores_df))\n",
        "if not scores_df.empty:\n",
        "    display(scores_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d831c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best config by MAPE (default criterion)\n",
        "if scores_df.empty:\n",
        "    print('No scores found yet.')\n",
        "else:\n",
        "    best_rows = []\n",
        "    for (ds, model, w), g in scores_df.groupby(['dataset_tag', 'model', 'window_size'], as_index=False):\n",
        "        pick = g.sort_values(['tune_MAPE', 'tune_R2', 'config_index'], ascending=[True, False, True], kind='stable').iloc[0]\n",
        "        best_rows.append(pick.to_dict())\n",
        "\n",
        "    best_df = pd.DataFrame(best_rows).sort_values(['dataset_tag', 'window_size', 'model'], kind='stable')\n",
        "    display(best_df[['framework', 'dataset_tag', 'model', 'window_size', 'config_index', 'tune_MAPE', 'tune_R2', 'eval_MAPE', 'eval_R2', 'hyperparams_json']])\n",
        "\n",
        "    out_scores = Path('results') / f'index_config_scores_{FRAMEWORK_NAME}.csv'\n",
        "    out_best = Path('results') / f'index_best_configs_{FRAMEWORK_NAME}.csv'\n",
        "    out_scores.parent.mkdir(parents=True, exist_ok=True)\n",
        "    scores_df.to_csv(out_scores, index=False)\n",
        "    best_df.to_csv(out_best, index=False)\n",
        "    print('[INFO] wrote', out_scores)\n",
        "    print('[INFO] wrote', out_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ff3183",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick visual summary\n",
        "if scores_df.empty:\n",
        "    print('No scores available.')\n",
        "else:\n",
        "    sns.set_theme(style='whitegrid')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n",
        "    bdf = scores_df.copy()\n",
        "    bdf['label'] = bdf['dataset_tag'] + '-' + bdf['model'] + '-w' + bdf['window_size'].astype(str)\n",
        "\n",
        "    m1 = bdf.sort_values('eval_MAPE', kind='stable')\n",
        "    axes[0].barh(m1['label'], m1['eval_MAPE'])\n",
        "    axes[0].set_title(f'{FRAMEWORK_NAME.upper()} Eval MAPE')\n",
        "    axes[0].set_xlabel('MAPE (%)')\n",
        "\n",
        "    m2 = bdf.sort_values('eval_R2', kind='stable')\n",
        "    axes[1].barh(m2['label'], m2['eval_R2'])\n",
        "    axes[1].set_title(f'{FRAMEWORK_NAME.upper()} Eval R2')\n",
        "    axes[1].set_xlabel('R2')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d4a315",
      "metadata": {},
      "source": [
        "## Visuals\n",
        "\n",
        "This section generates paper-ready figures and saves them under `results/figures_<framework>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d10c5659",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visuals for reporting\n",
        "if scores_df.empty:\n",
        "    print('No scores available.')\n",
        "else:\n",
        "    import matplotlib.ticker as mtick\n",
        "\n",
        "    fig_dir = Path('results') / f'figures_{FRAMEWORK_NAME}'\n",
        "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plot_df = scores_df.copy()\n",
        "    for c in ['tune_MAPE', 'tune_R2', 'eval_MAPE', 'eval_R2', 'window_size', 'config_index']:\n",
        "        plot_df[c] = pd.to_numeric(plot_df[c], errors='coerce')\n",
        "\n",
        "    # Best-by-tuning-MAPE table (default selection criterion)\n",
        "    best_rows = []\n",
        "    for (ds, model, w), g in plot_df.groupby(['dataset_tag', 'model', 'window_size'], as_index=False):\n",
        "        g = g.dropna(subset=['tune_MAPE'])\n",
        "        if g.empty:\n",
        "            continue\n",
        "        pick = g.sort_values(['tune_MAPE', 'tune_R2', 'config_index'], ascending=[True, False, True], kind='stable').iloc[0]\n",
        "        best_rows.append(pick.to_dict())\n",
        "    best_plot_df = pd.DataFrame(best_rows)\n",
        "\n",
        "    if best_plot_df.empty:\n",
        "        print('No valid rows after filtering for visuals.')\n",
        "    else:\n",
        "        sns.set_theme(style='whitegrid', context='talk')\n",
        "\n",
        "        # 1) Heatmap: eval MAPE for best tuned config per model/dataset/window\n",
        "        for ds, g in best_plot_df.groupby('dataset_tag'):\n",
        "            pv = g.pivot_table(index='model', columns='window_size', values='eval_MAPE', aggfunc='mean')\n",
        "            plt.figure(figsize=(7, 5))\n",
        "            ax = sns.heatmap(pv, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'Eval MAPE (%)'})\n",
        "            ax.set_title(f'{FRAMEWORK_NAME.upper()} {ds}: Eval MAPE Heatmap (best by tune MAPE)')\n",
        "            ax.set_xlabel('Window Size')\n",
        "            ax.set_ylabel('Model')\n",
        "            plt.tight_layout()\n",
        "            out = fig_dir / f'heatmap_eval_mape_{ds.lower()}.png'\n",
        "            plt.savefig(out, dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "        # 2) Grouped bars: eval MAPE by model/window for each dataset\n",
        "        for ds, g in best_plot_df.groupby('dataset_tag'):\n",
        "            plt.figure(figsize=(11, 5))\n",
        "            ax = sns.barplot(data=g, x='model', y='eval_MAPE', hue='window_size', palette='Set2')\n",
        "            ax.set_title(f'{FRAMEWORK_NAME.upper()} {ds}: Eval MAPE by Model and Window')\n",
        "            ax.set_xlabel('Model')\n",
        "            ax.set_ylabel('Eval MAPE (%)')\n",
        "            ax.legend(title='Window')\n",
        "            plt.xticks(rotation=20)\n",
        "            plt.tight_layout()\n",
        "            out = fig_dir / f'bar_eval_mape_{ds.lower()}.png'\n",
        "            plt.savefig(out, dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "        # 3) Tuning-vs-eval generalization scatter\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        ax = sns.scatterplot(\n",
        "            data=plot_df,\n",
        "            x='tune_MAPE',\n",
        "            y='eval_MAPE',\n",
        "            hue='model',\n",
        "            style='dataset_tag',\n",
        "            size='window_size',\n",
        "            sizes=(40, 180),\n",
        "            alpha=0.75,\n",
        "        )\n",
        "        ax.set_title(f'{FRAMEWORK_NAME.upper()}: Tune vs Eval MAPE (all configs)')\n",
        "        ax.set_xlabel('Tune MAPE (%)')\n",
        "        ax.set_ylabel('Eval MAPE (%)')\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        out = fig_dir / 'scatter_tune_vs_eval_mape.png'\n",
        "        plt.savefig(out, dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # 4) Frontier: Eval MAPE vs Eval R2 on best configs\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        ax = sns.scatterplot(\n",
        "            data=best_plot_df,\n",
        "            x='eval_MAPE',\n",
        "            y='eval_R2',\n",
        "            hue='model',\n",
        "            style='dataset_tag',\n",
        "            size='window_size',\n",
        "            sizes=(80, 220),\n",
        "            alpha=0.85,\n",
        "        )\n",
        "        for _, r in best_plot_df.iterrows():\n",
        "            ax.text(r['eval_MAPE'], r['eval_R2'], f\"{r['dataset_tag']}-w{int(r['window_size'])}\", fontsize=8, alpha=0.75)\n",
        "        ax.set_title(f'{FRAMEWORK_NAME.upper()}: Eval MAPE vs Eval R2 Frontier (best tuned configs)')\n",
        "        ax.set_xlabel('Eval MAPE (%)')\n",
        "        ax.set_ylabel('Eval R2')\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        out = fig_dir / 'frontier_eval_mape_vs_r2_best.png'\n",
        "        plt.savefig(out, dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # Save tables used for paper\n",
        "        best_out = fig_dir / 'best_plot_table.csv'\n",
        "        full_out = fig_dir / 'all_scores_table.csv'\n",
        "        best_plot_df.to_csv(best_out, index=False)\n",
        "        plot_df.to_csv(full_out, index=False)\n",
        "        print('[INFO] Saved figures and tables to', fig_dir)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.11.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
